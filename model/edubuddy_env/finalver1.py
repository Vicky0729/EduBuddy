# -*- coding: utf-8 -*-
"""finalver1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jt3q7JCsMVoSbhGZzkECGLAcFWQmoGMr
"""

## 원문텍스트 입력

original_text = """
안녕하세요,여러분.오늘 은우리 주변의 모든것이 무엇으로 이루어져있는 지 알아보 겠습니 다.여러 분,우리 주변 의모든 것은무엇으로이루어져있을까요?맞아 요, 바로원자 와분자예요.이시간에 는물질의 기본 단위인 원자 와분자 에대해 자세히 알아보도 록하겠습 니다.

먼저원 자에 대해이야기 해볼 까요?원자는모든물질의 기본 단위입니다.원자의구 조는 어떻게생겼 을까요?중앙 에핵이있고,그주위를전 자가 돌고있어요.핵은양성자 와중성자로 이루어져있고,전자는그주위를빠르게돌고있습니다.원 자의 크기는매우작아서눈 에보이지않아요.여러분이손 에들고있는 펜이나종이도모두원자로 이루어져있답니다.원자는종류에따라서로다른특성을가지고있 어요.

이제원자가어떻게결합 해서물질 을 이루는지 알아볼까요?여러원자가모여서분 자를이루죠.예를들어,물 은두개의수소원 자와하나의산소원자가결합해 이루어진분자예요.또다른예로,이산화탄소는한 개의탄소원 자와두개의산소 원자로 이루어져있 어요.이렇게원 자들이결합하여다양한물질 을 이루게됩 니다.

원소와화합 물이라는개념 도중요해요.원소는같은종류의원자로만 이루어진 물질이고,화합물은다른종류의원자가결합 한물질이에요.주기율표를 보면원소 들이정리되어있답니다.원소들은금속,비금속,준금속 으로구분되기도해요.예를 들어,금속 원소로는철,구리,알루미늄등이있고,비금속원소로는산소,질소,탄소 등이있 어요.

주기율표 에서수소는가장가벼운 원소로,첫번째위치 에있어요.헬륨 은두번째원소 로,매우안정한기체입니다.탄소 는모든생명체의기본구성 요소로매우중요한원소 입니다.산소는우리가숨을쉴때필요한원소로,물을구성하는중요한원소이기도해요.

여러분,원자가모여분 자를이루고,원소 와화합물이물질을구성한다는것을 이해하셨나요?원자와분자의개념이어렵게 느껴질수도있지만,실제로는우리주변의모든물질 이이러한기본 단위로이루어져있 답니다.

이제원자 와분자,원소 와화합물에 대해 알게되었죠?혹시질문이있나요?다음시간에는물질의상태 변화에대해 알아보겠습니다.감사합니다.
"""

## 띄어쓰기 교정
## 모델 : https://github.com/haven-jeon/PyKoSpacing

!pip install git+https://github.com/haven-jeon/PyKoSpacing.git

from pykospacing import Spacing

spacing = Spacing()
kospacing_text = spacing(original_text)

print(kospacing_text)

"""▲ 여기까지 띄어쓰기 교정

// 형태소 분석 모델
https://github.com/bab2min/kiwipiepy
"""

! pip install --upgrade pip
! pip install kiwipiepy

from kiwipiepy import Kiwi
kiwi = Kiwi()
secntencs = kiwi.split_into_sents(kospacing_text)

secntencs[1]

sentences = [sent.text for sent in kiwi.split_into_sents(kospacing_text)]

from sentence_transformers import SentenceTransformer, util

# 2. Sentence-BERT 모델 로드
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

# 3. 과학 키워드와 문장 임베딩 계산
## keywords는 추후 github랑 연결된 키워드 사용예정
keywords = ["과학", "물질", "원소", "우주", "연구", "에너지"]
keyword_embedding = model.encode(keywords)
sentence_embeddings = model.encode(sentences)

# 4. 각 문장의 키워드 유사도 계산
filtered_sentences = []
for i, sent_emb in enumerate(sentence_embeddings):
    similarity_scores = util.cos_sim(sent_emb, keyword_embedding).mean()
    if similarity_scores > 0.2:  # 유사도 기준(조정 가능)
        filtered_sentences.append(sentences[i])

# 결과 출력
print("과학 관련 문장:")
print(filtered_sentences)

# 각 문장에서 명사만 추출
nouns = []

for sentence in filtered_sentences:
    # 문장 토큰화하여 품사 정보 포함
    tokens = kiwi.tokenize(sentence, normalize_coda=True)
    # 명사만 추출 (NNG: 일반 명사, NNP: 고유 명사, NNB: 의존 명사 등)
    sentence_nouns = [token.form for token in tokens if token.tag in ['NNG', 'NNP', 'NNB']]
    nouns.append(sentence_nouns)

# 결과 출력
for idx, sentence_nouns in enumerate(nouns):
    print(f"문장 {idx + 1}: {sentence_nouns}")

from kiwipiepy.utils import Stopwords

kiwi = Kiwi()
stopwords = Stopwords()

# 추가할 내용 Github랑 연결예정
custom_stopwords = ['안녕하세요', '감사', '이야기', '질문']

# 각 문장에서 명사만 추출
nouns = []

for sentence in filtered_sentences:
    # 문장 토큰화하여 품사 정보 포함
    tokens = kiwi.tokenize(sentence, normalize_coda=True)
    # 명사만 추출 (NNG: 일반 명사, NNP: 고유 명사, NNB: 의존 명사 등)
    sentence_nouns = [token.form for token in tokens if token.tag in ['NNG', 'NNP', 'NNB']]

    # 사용자 정의 불용어만 제외
    sentence_nouns = [noun for noun in sentence_nouns if noun not in custom_stopwords]

    # 기본 불용어는 자동으로 제외됨
    # Stopwords expects (form, tag) tuple, so modify the list comprehension
    sentence_nouns = [noun for noun in sentence_nouns if (noun, None) not in stopwords]
    # We use (noun, None) since we only have the word and not its tag.

    nouns.append(sentence_nouns)

# 결과 출력
for idx, sentence_nouns in enumerate(nouns):
    print(f"문장 {idx + 1}: {sentence_nouns}")

nouns

!pip install keybert

"""// TF-IDF로 키워드 추출"""

from keybert import KeyBERT
from sklearn.feature_extraction.text import TfidfVectorizer

# KeyBERT 모델 초기화
kw_model = KeyBERT()

# 명사만 합쳐서 하나의 문서로 만들기
nouns_text = [" ".join(sentence_nouns) for sentence_nouns in nouns]

# TF-IDF Vectorizer 초기화
vectorizer = TfidfVectorizer()

# TF-IDF 계산
tfidf_matrix = vectorizer.fit_transform(nouns_text)

# KeyBERT를 사용해 중요 키워드 추출
keywords = kw_model.extract_keywords(
    " ".join(nouns_text),  # 전체 명사 텍스트를 하나로 합쳐서 전달
    top_n=5,  # 상위 5개 키워드 추출
    vectorizer=vectorizer,  # TF-IDF Vectorizer를 전달
    seed_keywords=None
)

# 결과 출력
for idx, (keyword, score) in enumerate(keywords):
    print(f"{idx + 1}. {keyword} (score: {score})")

# 키워드 추출 결과
keywords_list = [keyword[0] for keyword in keywords]

# JSON 형태로 변환
data = {
    'filtered_sentences': filtered_sentences,
    'keywords': keywords_list
}

"""// mySQL연결"""

